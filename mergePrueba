; void ASM_merge1(uint32_t w, uint32_t h, uint8_t* data1, uint8_t* data2, float value)
global ASM_merge1
ASM_merge1:

;value--> xmm0

mov r12D, edi
mov r13D, esi
mov r14, rdx
mov r15, rcx

xorps xmm5, xmm5

;mov rcx, w*h cantidad de pixeles

xorps xmm1, xmm1        ; xmm1 = 0 | 0 | 0 | 0
addss xmm1, xmm0        ; xmm1 = 0 | 0 | 0 | value
pslldq xmm1, 4          ; xmm1 = 0 | 0 | value | 0
addss xmm1, xmm0        ; xmm1 = 0 | 0 | value | value
pslldq xmm1, 4          ; xmm1 = 0 | value | value | 0
addss xmm1, xmm0        ; xmm1 = 0 | value | value | value
pslldq xmm1, 4          ; xmm1 = value | value | value | 0
addss xmm1, xmm0        ; xmm1 = value | value | value | value




.ciclo:

	movups xmm3, [r14]
	movdqu xmm4, xmm3
	punpcklbw xmm4, xmm5  ; extendemos a 16 bits los 8 numeros de la parte baja.
	movdqu xmm6, xmm4     ; los 8 numeros de la parte baja en 32 bit
	punpcklwd xmm4,xmm5   ; extendemos a 32 bits los 4 numeros de la parte baja-baja.
	punpckhwd xmm6,xmm5   ; extendemos a 32 bits los 4 numeros de la parte baja-alta.


	movdqu xmm7, xmm3
	punpckhbw xmm7, xmm5  ; extendemos a 16 bits los 8 numeros de la parte alta.
	movdqu xmm8, xmm7     ; los 8 numeros de la parte alta en 32 bit
	punpcklwd xmm7,xmm5   ; extendemos a 32 bits los 4 numeros de la parte alta-baja.
	punpckhwd xmm8,xmm5   ; extendemos a 32 bits los 4 numeros de la parte alta-alta.

	;falta aplicar mascara para no variar la transparencia
	
	
	cvtdq2ps xmm4,xmm4    ; convertimos a float		
	mulps xmm4,xmm1       ; multiplicamos por value
	cvtps2dq xmm4, xmm4   ; lo volvemos a convertir en enteros de 32

	packssdw xmm4,xmm4    ; xmm4 = primeros 4 resultados
	packuswb xmm4,xmm4    ; los devolvemos a byte

	xorps xmm9, xmm9        ; xmm9 = 0 | 0 | 0 | 0
	paddb xmm9, xmm4        ; xmm9 = 0 | 0 | 0 | baja-baja
	pslldq xmm9, 4          ; xmm9 = 0 | 0 | baja-baja | 0
	paddb xmm9, xmm6        ; xmm9 = 0 | 0 | baja-baja | baja-alta
	pslldq xmm9, 4          ; xmm9 = 0 | baja-baja | baja-alta | 0
	paddb xmm9, xmm7        ; xmm9 = 0 | baja-baja | baja-alta | alta-baja
	pslldq xmm9, 4          ; xmm9 = baja-baja | baja-alta | alta-baja | 0
	paddb xmm9, xmm8        ; xmm9 = baja-baja | baja-alta | alta-baja | alta-alta



	movups [r14], xmm9
	add r14, 16
	
	loop .ciclo

 ret
